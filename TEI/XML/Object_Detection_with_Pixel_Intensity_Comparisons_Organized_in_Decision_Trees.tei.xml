<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Detection with Pixel Intensity Comparisons Organized in Decision Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-08-20">August 20, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nenad</forename><surname>Markuš</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<addrLine>Unska 3</addrLine>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miroslav</forename><surname>Frljak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<addrLine>Unska 3</addrLine>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Igor</forename><forename type="middle">S</forename><surname>Pandžić</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering and Computing</orgName>
								<orgName type="institution">University of Zagreb</orgName>
								<address>
									<addrLine>Unska 3</addrLine>
									<postCode>10000</postCode>
									<settlement>Zagreb</settlement>
									<country key="HR">Croatia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jörgen</forename><surname>Ahlberg</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<postCode>SE-581 83</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Forchheimer</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Linköping University</orgName>
								<address>
									<postCode>SE-581 83</postCode>
									<settlement>Linköping</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Object Detection with Pixel Intensity Comparisons Organized in Decision Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-08-20">August 20, 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">F43BB5543B9EB9915E27568CF9B3CD03</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-15T09:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We describe a method for visual object detection based on an ensemble of optimized decision trees organized in a cascade of rejectors. The trees use pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast. Experimental analysis is provided through a face detection problem. The obtained results are encouraging and demonstrate that the method has practical value. Additionally, we analyse its sensitivity to noise and show how to perform fast rotation invariant object detection. Complete source code is provided at https://github.com/nenadmarkus/pico.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In computer vision, detection can be described as a task of finding the positions and scales of all objects in an image that belong to a given appearance class. For example, these objects could be cars, pedestrians or human faces. Automatic object detection has a broad range of applications. Some include biometrics, driver assistance, visual surveillance and smart human-machine interfaces. These applications create a strong motivation for the development of fast and accurate object detection methods.</p><p>Viola and Jones <ref type="bibr" target="#b19">[20]</ref> have made object detection practically feasible in real world applications. This is due to the fact that systems based on their framework can process images much faster than previous approaches while achieving similar detection results. Still, some applications could benefit from faster detectors, and this was the main motivation for our research. We are interested in supporting a wide range of PC and mobile devices with limited processing power. Thus, to make our system practical in these applications, we are ready to sacrifice detection accuracy at the expense of better processing speeds and simplicity.</p><p>In this paper, we describe an object detection framework which is able to process images very fast while retaining competitive accuracy. Basic ideas are described in Section 2. Experimental analysis is provided in Section 3. Section 4 summarizes our findings and discusses future research directions.</p><p>We have noticed (Feb. 2014) that an almost identical framework has been described by Liao et al. in a technical report <ref type="bibr" target="#b10">[11]</ref>. Here we acknowledge their work and state that our research has been performed completely independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our approach is a modification of the standard Viola-Jones object detection framework. The basic idea is to scan the image with a cascade of binary classifiers at all reasonable positions and scales. An image region is classified as an object of interest if it successfully passes all the members of the cascade. Each binary classifier consists of an ensemble of decision trees with pixel intensity comparisons as binary tests in their internal nodes. The learning process consists of a greedy regression tree construction procedure and a boosting algorithm. The details are given in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Decision tree for image based regression</head><p>To address the problem of image based regression, we use an optimized binary decision tree with pixel intensity comparisons as binary tests in its internal nodes. This approach was introduced by Amit and Geman in <ref type="bibr" target="#b0">[1]</ref>, and later successfully used by other researchers and engineers (for example, see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13]</ref>). A pixel intensity comparison binary test on image I is defined as bintest</p><formula xml:id="formula_0">(I; l 1 , l 2 ) = 0, I(l 1 ) ≤ I(l 2 ) 1, otherwise,<label>(1)</label></formula><p>where I(l i ) is the pixel intensity at location l i . Locations l easily be resized if needed. Each terminal node of the tree contains a scalar that models the output. The construction of the tree is supervised. Training data is a set {(I s , v s , w s ) : s = 1, 2, . . . , S} where v s is the ground truth value associated with image I s and w s is its importance factor (weight). For example, in the case of binary classification, the ground truths represent class labels: Positive samples are annotated with +1 and negative samples with −1. The weights w s enable us to assign different importance value to each of these samples. This will prove important later. The binary test in each internal node of the tree is selected in a way to minimize the weighted mean squared error obtained when the incoming training data is split by the test. This is performed by minimizing the following quantity:</p><formula xml:id="formula_1">WMSE = (I,v,w)∈C0 w • (v − v0 ) 2 + (I,v,w)∈C1 w • (v − v1 ) 2 , (2)</formula><p>where C 0 and C 1 are clusters of training samples for which the results of binary test on an associated image were 0 and 1, respectively. Scalars v0 and v1 are weighted averages of ground truths in C 0 and C 1 , respectively. As the set of all pixel intensity comparisons is prohibitively large, we generate only a small subset during optimization of each internal node by repeated sampling of two locations from a uniform distribution on a square [−1, +1] × [−1, +1]. The test that achieves the smallest error according to equation 2 is selected. The training data is recursively clustered in this fashion until some termination condition is met. In our setup, we limit the depth of our trees to reduce training time, runtime processing speed and memory requirements. The output value associated with each terminal node is obtained as the weighted average of ground truths that arrived there during training.</p><p>If we limit the depth of the tree to D and we consider B binary tests at each internal node, the training time is O(D • B • S) for a training set containing S samples, i.e., it is linear in all relevant parameters. This follows from the observation that each training sample is tested with B pixel intensity comparisons for each internal node it visits on its path of length D from the root node to its terminal node. A constructed tree needs O(2 D ) bytes for storage and its runtime speed scales as O(D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">An ensemble of trees for binary classification</head><p>A single decision tree can usually reach only moderate accuracy. On the other hand, an ensemble of trees can achieve impressive results. We use the GentleBoost algorithm <ref type="bibr" target="#b3">[4]</ref>, a modification of the better known AdaBoost, to generate a discriminative ensemble by sequentially fitting a decision tree to an appropriate least squares problem.</p><p>In order to generate an ensemble of K trees from a training set {(I s , c s ) : s = 1, 2, . . . , S}, the algorithm proceeds in the following steps:</p><p>1. Initialize the weight w s for each image I s and its class label c s ∈ {−1, +1} as</p><formula xml:id="formula_2">w s = 1/P, c s = +1 1/N, c s = −1</formula><p>where P is the total number of positive samples and N is the total number of negative samples.</p><p>2. For k = 1, 2, . . . , K :</p><p>(a) Fit a decision tree T k by weighted least squares of c s to I s with weights w s (as explained in section 2.1).</p><p>(b) Update weights:</p><formula xml:id="formula_3">w s = w s exp (−c s T k (I s )) ,</formula><p>where T k (I s ) denotes the real-valued output of tree T k on image I s .</p><p>(c) Renormalize weights so they sum to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Output ensemble {T</head><formula xml:id="formula_4">k : k = 1, 2, . . . , K}.</formula><p>During runtime, the outputs of all trees in the ensemble are summed together and the obtained value is thresholded in order to obtain a class label. We can achieve different ratios of true positives to false positives by varying the threshold. This proves important in building an efficient detector, as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detection as image scanning with a cascade of binary classifiers</head><p>Without any a priori knowledge, we have to systematically scan the image with our binary classifier at all different positions and scales in order to find the objects of interest. As this is computationally demanding, we follow the proposal of Viola and Jones. The basic idea is to use multiple classification stages with increasing complexity. Each stage detects almost all objects of interest while rejecting a certain fraction of non-objects. Thus, the majority of non-objects are rejected by early stages, i.e., with little processing time spent. In our case, each stage consists of an ensemble of trees. Early stages have fewer trees than the later ones. The detection rate of each stage is regulated by adjusting the output threshold of its ensemble. Each stage uses the soft output ("confidence") of the previous stage as additional information to improve its discriminability. This is achieved by progressively accumulating the outputs of all classification stages in the cascade (similar to <ref type="bibr" target="#b20">[21]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Clustering raw detections</head><p>As the final detector is robust to small perturbations in position and scale, we expect multiple detections around each object of interest. These overlapping detections are clustered together in a post-processing step as follows.</p><p>We construct an undirected graph in which each vertex corresponds to a raw detection. Two vertices are connected if the overlap of their corresponding detections is greater than 30%:</p><formula xml:id="formula_5">D 1 ∩ D 2 D 1 ∪ D 2 &gt; 0.3.<label>(3)</label></formula><p>Next, we use the depth-first search to find the connected components in this graph. Raw detections within each component are combined in a single detection by averaging the position and scale parameters. This simple clustering procedure does not require the number of clusters to be set in advance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Face detection experiments</head><p>It is not obvious that the described framework can give pleasing results in practice. Experimental analysis is provided through the face detection problem. A thorough survey of the field can be found in <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training</head><p>For the purpose of training, we use the AFLW <ref type="bibr" target="#b8">[9]</ref> dataset and the one provided by Visage Technologies (http://www. visagetechnologies.com/). Both consist of face images annotated with approximate locations of the eyes. These are used to estimate the position and scale of each face. We extract around 20 000 frontal faces from these datasets and generate 15 positive training samples from each frontal face by small random perturbations on scale and position. We have observed in our preliminary experiments that this makes the detector more robust to aliasing and noise. Overall, this results in around 300 000 positive samples. For learning of each stage we extract 300 000 negatives from a large set of images that do not contain any faces by collecting the regions that were not discarded by any of the previously learned stages.</p><p>The parameters of the learning process have to be set in advance. In our experiments, we tune them to produce a detector which is able to process images rapidly, as this is our main goal. We fix the depth of each tree to 6 and use 20 classification stages. Each classification stage has a predefined number of trees and detection rate. We consider 256 binary tests during the optimization of each internal tree node. This optimization process considerably improves the performance of the cascade. For example, the first stage consists of a single tree. Its parameterized receiver operator characteristic (ROC) curve can be seen in Figure <ref type="figure" target="#fig_0">1</ref>. This experiment also implies that using randomized ferns <ref type="bibr" target="#b14">[15]</ref> in this framework leads to inferior processing speeds at runtime since ferns discard less negatives for the same stage complexity and detection rate. Similar conclusions can be made for other stages of the cascade. Some parameters and results of the learning process are reported in Table <ref type="table" target="#tab_1">1</ref>. The overall detection rate on the training set is approximately 0.92 for the estimated false positive rate of 10 −7 . Note that this apparently low detection rate does not mean poor performance in practice since we generated 15 randomized samples for each frontal face image in the available datasets. Also, we use the scanning window approach during runtime and expect multiple detections for each encountered face.</p><p>The learning takes around 30 hours on a modern PC with 4 cores and 16GB of RAM. Most of the computation effort goes to the sampling of negatives for learning of each new classification stage. The learned cascade uses less than 200 kB of storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of accuracy and processing speed</head><p>To put our system into perspective, we compare its detection rate, false positive rate and processing speed to the triedand-true face detection implementations available in OpenCV (http://opencv.org/, version 2.4.3). The first one is based on Haar-like features (standard Viola-Jones framework, see <ref type="bibr" target="#b11">[12]</ref>) and the other on local binary patterns (LBPs, see <ref type="bibr" target="#b23">[24]</ref>). We would like to note that there are certain limitations to the experiments that follow and we will not be able to provide absolute measures how the methods compare. The idea is to compare the implementation of our method to the baseline provided by OpenCV.</p><p>To evaluate the detection rates, we use the GENKI-SZSL <ref type="bibr" target="#b18">[19]</ref> and CALTECH-FACES <ref type="bibr" target="#b1">[2]</ref> datasets. The datasets contain 3500 and 10 000 annotated faces, respectively. We report the number of false positives on two large sets of images that  do not contain any faces, NO-FACES-1 and NO-FACES-2. All detectors start the scan with a 24 × 24 pixel window and enlarge it by 20% until it exceeds the size of the image. For a given scale, our system scans the image by shifting the window by 10% of its size. The ROC curves can be seen in Figures <ref type="figure" target="#fig_3">2 and 3</ref> (circular markers represent the recommended operating points for OpenCV detectors in real-time applications, as found in the documentation). We conclude that our system slightly outperforms the other two detectors in terms of accuracy. Of course, there is always the problem of dataset bias <ref type="bibr" target="#b17">[18]</ref> and unknown implementation details, i.e., we cannot conclude which method is more accurate in general situations based just on these limited experiments.</p><p>The V-J detector scans the NO-FACES-1 set in 602 seconds and the LBP-based detector in 240 seconds. Our system needs 111 seconds for the task. The reported times are on a 2.5GHz machine. We are interested in a more realistic setup: Scan a 640 × 480 image starting with a 100 × 100 window that is enlarged by 20% until it exceeds the size of the image. This situation is commonly encountered in real-world applications such as video conferencing or face tracker initialization. The processing speeds are reported in Table <ref type="table" target="#tab_2">2</ref>. Bear in mind that the implementation available in OpenCV is highly optimized for PCs, i.e., it uses SIMD instructions, multi-core processing and cache-optimized data structures. Its poor performance on mobile devices can be explained by limited hardware support  for floating point operations on ARM architectures<ref type="foot" target="#foot_0">1</ref> . Our implementation is written in pure C, without much time spent on tweaking for processing speed. Also, all processing is done in a single thread, i.e., uses a single CPU core.</p><p>We conjecture that it is possible to obtain even better results with more advanced cascade construction/optimization techniques (for example, see <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>). This kind of experiments were beyond the scope of the present paper and we plan to perform some of them in the future. Also, note that we can increase the accuracy of our face detector at the cost of processing speed by scanning the images more densely, for example, by enlarging the scanning window by 10% instead of 20% when changing the detection scale. The choice of these parameters depends on the targeted application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with other methods</head><p>A common face detection accuracy benchmark is the FDDB dataset <ref type="bibr" target="#b5">[6]</ref>. It contains 5171 faces acquired under unconstrained conditions. Some annotated faces are in out-ofplane rotated positions and frontal face detectors will not be able to find them. As we do not have access to the source code/binaries of the face detection systems considered to be state-of-the-art, we rely on the results presented in the pa-   pers describing them. Some are summarized at http:// vis-www.cs.umass.edu/fddb/results.html and the other were obtained by visual inspection of the available accuracy plots. We use the protocol from <ref type="bibr" target="#b5">[6]</ref> to compare our system to the methods by Jun et al. <ref type="bibr" target="#b7">[8]</ref>, Li et al. <ref type="bibr" target="#b9">[10]</ref>, Jain et al. <ref type="bibr" target="#b6">[7]</ref> and a commercial system provided by Illux-Tech (http://illuxtech.com/). The discrete and continuous ROC curves can be seen in figures 4 and 5.</p><p>We can see that <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> and IlluxTech outperform our system on this dataset in terms of accuracy. Li et al. <ref type="bibr" target="#b9">[10]</ref> and Jun et al. <ref type="bibr" target="#b7">[8]</ref> compare the processing speeds of their systems to the V-J frontal face detector from OpenCV. All measurements were performed on modern personal computers. It is difficult to conjecture from the available data how the mentioned systems compare to ours because we do not know which parameters their authors used for the V-J detector in their experiments. If we assume that they used the same parameters as we did, this makes our system faster than Li et al. <ref type="bibr" target="#b9">[10]</ref> by approximately 4.5 times, and at the same speed as Jun et al. <ref type="bibr" target="#b7">[8]</ref>.</p><p>Some of object detection research focuses on building ever more accurate systems, even at the expense of processing speed (for example, deformable part-based models with gradient orientation histogram features, see <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>). As far as we know, the system presented by Yan et al. <ref type="bibr" target="#b21">[22]</ref> obtains  state-of-the-art face detection accuracy results on FDDB. The system uses multiple detectors, each trained to detect faces in a predefined range of out-of-plane rotations. Thus, we did not include these results in figures 4 and 5. The authors report a processing speed of around 20 frames per second for frontal face detection in 640 × 480 images on a modern personal computer. This suggests that their system would not achieve acceptable performance on mobile devices and other hardware with limited processing power. More in depth comparisons are not possible since we do not have access to the implementation of their method. Also, we conjecture that neural network-based object detection systems will obtain even better results in the future as neural networks started to outperform other machine learning methods on common benchmarks <ref type="bibr" target="#b4">[5]</ref>. However, neural networks are usually slow at runtime as they require a lot of floating point computations to produce their output, which is particularly problematic on mobile devices. To conclude, our opinion is that similar systems are not suitable for the same kind of applications as the face detector described in this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sensitivity to noise</head><p>It is reasonable to assume that our system performs poor in the presence of high noise levels due to simplicity of the used binary tests. On the other hand, the features used by OpenCV detectors should be robust in these circumstances as they are based on region averaging, which corresponds to lowpass filtering. We use the additive (uncorrelated) Gaussian noise model to quantitatively evaluate these effects: A sample from a Gaussian distribution with zero mean and standard deviation σ is added to the intensity of each image pixel. An experiment on the GENKI-SZSL dataset is reported in Figure <ref type="figure" target="#fig_9">6</ref>. We see that the detection rate of our system degrades significantly as the noise intensity increases. These adverse effects can be reduced by applying a low-pass filter prior to detection. We have not found this to be necessary as the noise levels in this experiment are uncommon for modern cameras, even on mobile devices. The images in the GENKI-SZSL dataset are already noisy and contain compression artefacts, i.e., they are representative of the ones encountered in realworld face detection applications.</p><p>Note that the presented experiment indicates that other systems based on similar features could be sensitive to high noise levels as well. An example of such a commercial system is the Microsoft Kinect human pose recognizer, described in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Detecting rotated faces</head><p>In some applications we are interested in object detection under general planar rotations. A simple solution is to scan the image at multiple orientations. In our case, this can be performed without image resampling as pixel intensity comparison binary tests can be easily rotated for a given angle (in our implementation, we use precomputed look-up tables as this  Table <ref type="table">3</ref>: Average times required to process a 640 × 480 pixel image at n orientations, attempting to find faces larger than 100 × 100 pixels at all possible planar rotations.</p><p>proved faster than evaluating trigonometric functions at runtime). It is not immediately clear if this leads to acceptable performance since it could result in poor processing speeds and/or large number of false positives. We use the previously learned face detector and provide experimental analysis.</p><p>To investigate the detection accuracy, we rotate each image found in the GENKI-SZSL dataset for a random angle sampled uniformly from the [0, 2π) interval. We report false positives on the NO-FACES-1 set. Results can be seen in Figure <ref type="figure" target="#fig_11">7</ref>. By comparing the ROC curves to the ones in Figure <ref type="figure" target="#fig_1">2</ref> we can see that the accuracy of the approach is comparable to the OpenCV's LBP-based frontal face detector. Processing speeds can be seen in Table <ref type="table">3</ref>. These results demonstrate that our system can perform rotation invariant face detection with reasonable accuracy in real-time using a single core of a modern personal computer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Qualitative results</head><p>Some qualitative results obtained by our system can be seen in Figure <ref type="figure" target="#fig_12">8</ref>. A video demonstrating rotation invariant face detection is available at http://youtu.be/1lXfm-PZz0Q. Furthermore, for readers who wish to test the method themselves, demo applications can be downloaded from http: //public.tel.fer.hr/odet/. Complete source code is provided at https://github.com/nenadmarkus/pico. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper we have shown that an object detection system based on pixel intensity comparisons organized in decision trees can achieve competitive results at high processing speed. This is especially evident on devices with limited hardware support for floating point operations. This could prove important in the embedded system and mobile device industries because it can reduce hardware workload and prolong battery life.</p><p>Further advantages of our method are:</p><p>• The method does not require the computation of integral images, image pyramid, HOG pyramid or any other similar data structure.</p><p>• All binary tests in internal nodes of the trees are based on the same feature type. For comparison, Viola and Jones used 5 different types of Haar-like features to achieve their results.</p><p>• There is no need for image preprocessing prior to detection (such as contrast normalization, resizing, Gaussian smoothing or gamma correction).</p><p>• The method can easily be modified for fast detection of in-plane rotated objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The ROC curves of the first stage of the cascade for different number of binary tests considered in the optimization of each internal tree node, r. Circular marker represents the point on which the stage operates at runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ROC curves for the GENKI-SZSL/NO-FACES-1 experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ROC curves for the CALTECH-FACES/NO-FACES-2 experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Discrete ROC curves for different face detection systems on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Continuous ROC curves for different face detection systems on the FDDB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Detection rate on the GENKI-SZSL dataset for different noise levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ROC curves for different number of considered face orientations, n, during the scan of each image. Device Time [ms] n = 6 n = 8 n = 10 n = 12 PC1 15.8 20.6 25.8 31.25 PC2 21.6 26.8 34.5 42.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Some results obtained by our system on real-world images.</figDesc><graphic coords="7,407.26,476.06,92.77,91.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of trees, true positive rate (TPR) and estimated false positive rate (FPR) for some stages.</figDesc><table><row><cell>num. trees</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>20</cell><cell>. . .</cell><cell>20</cell><cell>20</cell></row><row><cell>TPR [%]</cell><cell cols="11">97.5 98.0 98.5 99.0 99.5 99.7 99.9 99.9 . . . 99.9 99.9</cell></row><row><cell>FPR [%]</cell><cell cols="11">46.4 32.3 20.5 35.4 44.7 36.8 29.5 31.6 . . . 55.2 57.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average times required to find faces larger than 100 × 100 pixels in a 640 × 480 image.</figDesc><table><row><cell>Device</cell><cell>CPU</cell><cell cols="3">Time [ms] Our detector V-J (OpenCV) LBPs (OpenCV)</cell></row><row><cell>PC1</cell><cell>3.4GHz Core i7-2600</cell><cell>2.4</cell><cell>16.9</cell><cell>4.9</cell></row><row><cell>PC2</cell><cell>2.53GHz Core 2 Duo P8700</cell><cell>2.8</cell><cell>25.4</cell><cell>6.3</cell></row><row><cell>iPhone 5</cell><cell>1.3GHz Apple A6</cell><cell>6.3</cell><cell>175.3</cell><cell>47.3</cell></row><row><cell>iPad 2</cell><cell>1GHz ARM Cortex-A9</cell><cell>12.1</cell><cell>347.6</cell><cell>103.5</cell></row><row><cell>iPhone 4S</cell><cell>800MHz ARM Cortex-A9</cell><cell>14.7</cell><cell>430.3</cell><cell>129.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We are not the first to notice this problem with OpenCV. For example, see http://www.computer-vision-software.com/blog/2009/04/ fixing-opencv/ (accessed</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_1">on 29th of October, 2013).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is partially supported by Visage Technologies AB, Linköping, Sweden, by the Ministry of Science, Education and Sports of the Republic of Croatia, grant number 036-0362027-2028 "Embodied Conversational Agents for Services in Networked and Mobile Environments" and by the European Union through ACROSS project, 285939 FP7-REGPOT-2011-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape quantization and recognition with randomized trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1545" to="1588" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pruning training sets for learning of object categories</title>
		<author>
			<persName><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abu-Mostafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust object detection via soft cascade</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Additive logistic regression: a statistical view of boosting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fddb: A benchmark for face detection in unconstrained settings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM- CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online domain adaptation of a pre-trained cascade of classifiers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust face detection using local gradient patterns and evidence accumulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3304" to="3316" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First IEEE International Workshop on Benchmarking Facial Image Analysis Technologies</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face detection using surf cascade</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unconstrained face detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">December 2012. 1</date>
		</imprint>
		<respStmt>
			<orgName>Michigan State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Empirical analysis of detection cascades of boosted classifiers for rapid object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lienhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pisarevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th DAGM-Symposium</title>
				<meeting>the 25th DAGM-Symposium</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Eye pupil localization with an ensemble of randomized trees</title>
		<author>
			<persName><forename type="first">N</forename><surname>Markuš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frljak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Pandžić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="578" to="578" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Empirical analysis of cascade deformable models for multi-view face detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Orozco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast keypoint recognition in ten lines of code</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ozuysal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Waldboost -learning for time constrained sequential detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sochman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="150" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The mplab genki database, genkiszsl subset</title>
		<ptr target="http://mplab.ucsd.edu" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno>I-511- I-518</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boosting chain learning for object detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time high performance deformable model for face detection in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A survey of recent advances in face detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face detection based on multi-block lbp representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Biometrics</title>
		<imprint>
			<biblScope unit="volume">4642</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="11" to="18" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
